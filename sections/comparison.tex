% !TEX root=../pldi2019.tex



\section{Intuition}
\label{sec:comparison}

This section gives an overview over the essential concepts of \TOP and how they are represented in \TOPHAT.
These concepts are: communication with the environment, communication between subsystems, sequential composition, concurrency, and synchronization.
These concepts can also be found in process algebras, which raises the question how \TOP and process algebras relate.
While introducing the concepts, we compare them with Hoare's Communicating Sequential Processes (\CSP).
The comparison is based on Hoare's book \cite{books/Hoare85CSP}.
We chose \CSP as an example to stand for the numerous process algebras in existence, which, as far as this section is concerned, are reasonably similar.

We provide comparisons based on two aspects.
First, we compare the scope of the languages, as intended by the authors.
Second, we focus on features common to \TOP and \CSP, and study their respective realization.
When we point out differences, we do not argue that the different realizations are incompatible.
As a matter of fact the primitives can certainly be expressed in terms of each other.
Instead, we point out how the systems emphasize certain points of view by choosing different building blocks to express the same concepts.



\paragraph{Scope}

The central objective of \TOP is to coordinate the collaboration between people who work together to reach a common goal.
\TOP is a language to specify workflows.
\TOP focuses on collaboration, and has combinators to express ways in which people can work together.
Tasks can be executed after each other, at the same time, or conditionally.
This motivates the task combinators for internal and external steps, parallel \emph{and} and \emph{or}, and choice.
To employ these patterns of collaboration, users need to communicate.
This motivates the different forms of communication: with the environment, along control flow, and across control flow.

The central objective of \CSP is to model the patters of behaviour of processes.
These patterns of behaviour manifest themselves in sequences of actions that processes can engage in.
\CSP is a language to specify synchronization patterns.

In \TOP, communication is a means to an end.
In \CSP, communication is the central matter of discourse.

\CSP has a formal semantics that allows various kinds of correctness proofs, including equality of processes, and adherence to a specification.
This allows applications in program correctness, proofs of deadlock freedom, liveness, or verification of protocols.

\TOP focuses less on formal correctness, and more on practical applicability.
It wants to be a language with intuitive semantics that facilitates communication between programmers and domain experts.
\TOP programs are supposed to hide implementation details from domain experts while containing enough information to allow automatic generation of executable applications, including user interfaces.

The big difference between \CSP and \TOP is the intended level of abstraction.
While \CSP is used to explicitly model low-level details about communication, \TOP wants to hide details and provide high-level communication mechanisms to the programmer.

Despite the different origins, \TOP and \CSP feature concepts that serve similar purposes.



\subsection{Communication}

Both \TOP and \CSP have two sorts of communication: with the environment, and between subsystems.
In both languages, communication with the environment is input and output, and blocks evaluation of programs until the environment sends or receives events.
In both languages, communication between subsystems does not block, but evaluates programs as far as possible until further external input is required.

\paragraph{Communication with the environment}
In \TOP, communication with the environment is represented by editors.
A task of the form $\Edit v$, where $v$ is of type $\tau$, can receive any change event with a value of type $\tau$ and additionally the special event $\Empty$ that empties the editor.
The environment can always inspect the current value of an editor.

Editors have no notion of continuation.
Their sole purpose is to interact with the environment, retaining the last value that has been sent to them.

Editors can be used for both input and output.
The interpretation of whether an editor represents input or output is left to the reader.
An empty editor is most commonly interpreted as a prompt to input data, while a filled editor can be seen either as outputting its value, or as an input that comes with a default value.

In \CSP, communication with the environment is represented by prefixing.
The process $(a \to P)$ can engage in action $a$, after which it continues as process $P$.
In general, if $B$ is a set of events and $P(x)$ is an expression that evaluates to a process for each $x \in B$, the process $(x:B \to P(x))$ can engage in any one of the actions in $B$, after which it continues as the process determined by $P(x)$.
Hoare does not specify the language in which $P(x)$ is to be expressed.
He seems to permit any kind of mathematical formula, or any programming language.

Prefixing has no direction of sending and receiving.
Hoare uses the neutral phrase \emph{engaging in an action}.
The interpretation of whether an action stands for input or output is left to the reader.
For example, a vending machine $P = (\textit{coin} \to \textit{choc} \to P)$ is to be interpreted as taking a coin as input and producing chocolate as output.

Sending and receiving of values is modelled by giving additional structure to the names of actions.
For example, an action name could be $\textit{in}.5$, which can be interpreted as inputting the value 5.

In \TOP, event values are typed to match the type of the receiving editor.
In \CSP, action names are essentially strings, subject to implicit restrictions.



\begin{example}[Vending machine]

This example demonstrates external communication and choice.

The following \CSP program models a vending machine that dispenses a small biscuit for one coin and a large biscuit for two coins. $$V = \textit{coin} \to (\textit{small} \to \text{STOP} \mid \textit{coin} \to \textit{large} \to \text{STOP})$$

A vending machine in \TOP that is close in spirit to the one above looks as follows.
\begin{TASK}
  enter Int >>? \n. if n == 1 then edit "small"
    else if n == 2 then edit "large" else fail
\end{TASK}
This machine does not exhibit exactly the same behaviour as the one in \CSP, nor is it intended to.
Instead, this machine demonstrates how the same choice of buying a small biscuit for one coin or a large biscuit for two coins is expressed in \TOP.

The machine asks the user to enter an amount of money.
This editor stands for a coin slot in a real machine that freely accepts and returns coins.
There is a continue button that is initially disabled.
When the user has input exactly 1 or 2 coins, the continue button becomes enabled.
When the user presses the continue button, the machine dispenses either a large or a small biscuit, depending on the amount of money.

\end{example}



\paragraph{Communication between subsystems}

\TOP has two methods for internal communication, both different from its meth\-od for external communication.
The step combinator represents data flow that follows control flow, and shared data represents data flow across control flow.

The step combinator in $t \Then c$ passes the value of $t$ to the continuation $c$.
Shared data sources are assignable references, whose changes in value are immediately visible to all tasks interested in them.

The semantics of \TOP requires all updates to shared data and all enabled internal steps to be processed before any further communication with the environment can take place.
This introduces the possibility of diverging computations, where a cyclic dependency between shared data causes internal steps to be taken continuously without ever being able to engage in external communication.

\CSP uses prefixing for both internal and external communication.
Parallel processes that all eventually are willing to engage in some action must wait for the others and then progress in one synchronized step.
This step is still visible to the environment, which means the processes can not engage in this action unless the environment is also willing to.
Communication becomes internal by using \emph{concealment}.

Concealment is an operator that hides a given set of actions from the environment, which changes the semantics of prefixing for the concealed actions.
Processes do not have to wait for the environment, they can just step on their own.
In fact, processes must take any concealed steps as soon as possible, before any further communication with the environment can take place.
This means internal steps have priority over external communication.
This introduces the possibility of diverging computations, that is processes which continuously take internal steps without ever being able to engage in external communication.



\paragraph{Sequential composition}

The intuition about sequential composition is to first do one thing, and once it is done, do some other thing.

In \TOP, tasks never terminate.
Nonetheless, the notion of sequential composition exists in two forms: the external and the internal step.
The task $t \Then c$ acts like $t$ as long as the step is guarded, as described in \autoref{sub:steps}.
Once it becomes unguarded, the task continues as $cv$, where $v$ is the value of $t$.
The task $t \Next c$ requires an input event $\Continue$ in addition to the step being unguarded in order to step.

In \CSP, sequential composition is represented by the semicolon combinator together with the special action $\checkmark$, called \emph{success}.
We require that if a process engages in $\checkmark$ at all, it must be its single last action.
When that happens, we say the process terminates successfully.
The process $P;Q$ behaves like $P$ until $P$ terminates successfully, after which it continues as $Q$.
If $P$ never terminates successfully, neither does $P;Q$

\CSP's semicolon is like an internal step $t \Then c$ in \TOP where $t$ uses $\Unit$ to signal when it is done, and $c$ never fails.



\subsection{Multiprogramming}

Both \TOP and \CSP are models of \emph{multiprogramming}, as opposed to multiprocessing.
This means that one processor evaluates multiple programs in an interleaving fashion.
Multiprogramming involves three aspects: concurrency, synchronization, and nondeterminism.

\paragraph{Concurrency}

Concurrency is the source of parallelism: two things are done at the same time.

In \TOP, the parallel combinator $t_1 \And t_2$ stands for two independent tasks carried out at the same time.
For the system it does not matter if the tasks are executed by two people actually in parallel, or by one person who switches between the tasks.
All events sent to the combined task are interleaved into a serial stream of inputs.
The tasks are truly independent of each other, all interleavings are permitted.
Events to $t_1$ and $t_2$ must be prefixed by $\Left$ and $\Right$ respectively.
This unambiguously renames the events, removing any source of nondeterminism.

In \CSP, there are two different combinators for parallel composition: parallel and interleave.
The parallel process $P || Q$ can take $P$-steps and $Q$-steps in arbitrary interleaving for actions unique to $P$ and $Q$.
Actions in the alphabets of both $P$ and $Q$ must be taken in synchronization, so if one process wants to take such a step, it must wait until the other is ready to do so.
Synchronized steps are the only occasion where two processes actually do something at the same time.

The interleaving operator $P ||| Q$ does not synchronize any actions of $P$ and $Q$.
All interleavings are permitted.
Actions that occur in both alphabets are nondeterministically taken by either $P$ or $Q$.



\paragraph{Synchronization}

Synchronization means that an agent has to pause execution until some condition is met.
The need for synchronization arises quite naturally in countless variations in situations involving concurrency.
For multiprogramming, synchronization always means that some of the possible interleavings are forbidden.

For example, the mutual exclusion problem for two parallel processes $a_p \to b_p \to \text{STOP} $ and $a_q \to b_q \to \text{STOP}$ can be stated as:
In no interleaving should there be two adjacent $a$'s.

The mutual exclusion problem for two parallel tasks can be stated as follows.
Let \textit{inc} and \textit{dec} be tasks that increment and decrement some shared counter.
In $(\textit{inc} \Next \textit{dec}) \And (\textit{inc} \Next \textit{dec})$, the shared counter should at no point be greater than 1.
This means that in no interleaving should there be two adjacent \textit{inc}'s.

The basic method for synchronization in \TOP is built into the step combinator.
The task $t \Next c$ can only continue execution when two conditions are met:
Task $t$ must have a value $v$, and $cv$ must not evaluate to $\Fail$.
Programmers can encode arbitrary conditions in $cv$, which are evaluated atomically between interaction steps.
This allows a variety of synchronization problems to be solved in an intuitive and straight-forward manner.

The basic method for synchronization in \CSP is synchronized prefixing.
If some action $a$ is in the alphabets of both $P$ and $Q$, then their parallel composition $P||Q$ can take an $a$-step only if both $P$ and $Q$ are ready to take an $a$-step, in which case they both take their $a$-step simultaneously.
They both have to wait for the condition that the other process is ready to take the step.
All synchronization problems in \CSP must be solved by employing synchronized steps in some form.



\begin{example}[Cigarette smokers]

The cigarette smokers problem \cite{books/Downey08LBOS} is a surprisingly tricky synchronization problem.
We study it here because it demonstrates the capabilities of guarded steps.
The problem requires waiting for two conditions, waking up only if both conditions are satisfied.
The problem is stated as follows.
In order to smoke a cigarette, three ingredients are required: tobacco, paper, and a match.
There are three smokers, each having one of the ingredients and requiring the other two.
There is an agent that randomly provides two of the ingredients.

Downey models availability of the ingredients using a semaphore for each ingredient.
The agent randomly signals two of the three semaphores.
The problem asks writing code for the smokers so that the smoker who requires the two signalled semaphores wakes up and proceeds with smoking.

A na\"ive attempt where each smoker first waits for one and then the other semaphore can lead to deadlock.
The following pseudo-code is a non-solution.
Let \emph{paper}, \emph{tobac} and \emph{match} be semaphores, initialized to 0.
\begin{TASK}
  tobacSmoker = wait(match); wait(paper); smoke();
  paperSmoker = wait(tobac); wait(match); smoke();
  matchSmoker = wait(tobac); wait(paper); smoke();
\end{TASK}
To see what can go wrong, imagine the three smokers running in parallel.
Let the agent signal the two semaphores \emph{tobac} and \emph{paper}.
One of \emph{paperSmoker} and \emph{matchSmoker} can take their first step, but if \emph{paperSmoker} takes the step, the system deadlocks.

The solution proposed by Downey involves an additional mutex, three additional semaphores, three additional threads called \emph{pushers}, and three regular Boolean variables.
The job of the pushers is to record availability of their ingredient in their Boolean variable, and check availability of other resources, waking the correct smoker when appropriate.
The details of the solution are not important here.

What is important is that the implementation of what is essentially waiting for two semaphores requires a substantial amount of additional synchronization, together with non-trivial conditional statements.

A similar effort as in Downey's solution must be made in order to solve the problem using synchronized actions in \CSP.
This is because in \CSP, processes can only synchronize on one action at a time.

\TOP allows a simple solution to this problem, using guarded steps.
Steps can be guarded with arbitrary expressions, and the parallel combinator can be used to watch two shared data sources at the same time.
Let \textit{match}, \textit{paper}, and \textit{tobac} be shared Booleans.
The smokers are defined as follows.
\begin{TASK}
  tobacSmoker = (update match <&> update paper) >>? continue
  paperSmoker = (update tobac <&> update match) >>? continue
  matchSmoker = (update tobac <&> update paper) >>? continue
  continue = \<<x,y>>. if x && y then smoke else fail
\end{TASK}
When the agent supplies two of the ingredients by setting the respective shares to $\True$, only the step of the smoker that waits for those becomes enabled.

\end{example}



\paragraph{Nondeterminism}

Nondeterminism means that a system can react to the same input in different ways, the choice of which can not be influenced by the environment.

\CSP has an operator to explicitly introduce nondeterminism.
Furthermore, nondeterminism can arise from the combination of some other combinators, for example choice and concealment.

Hoare states that nondeterminism is only useful for \emph{specifying} processes, never for \emph{implementing} them \cite{books/Hoare85CSP}.
\CSP can be used for both.
A process with nondeterministic behaviour must always be regarded as a specification, while a deterministic process can be seen as specification or implementation.

\TOP is meant solely for implementation and does not have any form of nondeterminism.
Input events for parallel tasks are disambiguated, internal steps have a well-defined evaluation order, and internal choice is left-biased.


\paragraph{Conclusion}

There are several concepts that exist in both languages, like concurrency, sequential composition, or sending values between concurrent agents.
In \TOP however they are meant to be used on a higher level of abstraction, which allows concise declarative description of patterns of collaboration, without worrying about the implementation details of communication.
